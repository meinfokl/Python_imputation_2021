{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attrition_Flag : 2\n",
      "Customer_Age : 45\n",
      "Gender : 2\n",
      "Dependent_count : 6\n",
      "Education_Level : 7\n",
      "Marital_Status : 4\n",
      "Income_Category : 6\n",
      "Card_Category : 4\n",
      "Months_on_book : 44\n",
      "Total_Relationship_Count : 6\n",
      "Months_Inactive_12_mon : 7\n",
      "Contacts_Count_12_mon : 7\n",
      "Credit_Limit : 6205\n",
      "Total_Revolving_Bal : 1974\n",
      "Avg_Open_To_Buy : 6813\n",
      "Total_Amt_Chng_Q4_Q1 : 1158\n",
      "Total_Trans_Amt : 5033\n",
      "Total_Trans_Ct : 126\n",
      "Total_Ct_Chng_Q4_Q1 : 830\n",
      "Avg_Utilization_Ratio : 964\n",
      "(10127, 20)\n",
      "(10127, 20)\n",
      "shape (10127, 20)\n",
      "train base gan\n",
      "Epoch 1/100\n",
      "30381/30381 [==============================] - 1s 46us/step - loss: 0.4698 - acc: 0.7309\n",
      "Epoch 2/100\n",
      "30381/30381 [==============================] - 1s 27us/step - loss: 0.3888 - acc: 0.7966\n",
      "Epoch 3/100\n",
      "30381/30381 [==============================] - 1s 27us/step - loss: 0.3301 - acc: 0.8398\n",
      "Epoch 4/100\n",
      "30381/30381 [==============================] - 1s 28us/step - loss: 0.2900 - acc: 0.8664\n",
      "Epoch 5/100\n",
      "30381/30381 [==============================] - 1s 29us/step - loss: 0.2552 - acc: 0.8871\n",
      "Epoch 6/100\n",
      "30381/30381 [==============================] - 1s 28us/step - loss: 0.2279 - acc: 0.9014\n",
      "Epoch 7/100\n",
      "30381/30381 [==============================] - 1s 29us/step - loss: 0.2141 - acc: 0.9085\n",
      "Epoch 8/100\n",
      "30381/30381 [==============================] - 1s 29us/step - loss: 0.2032 - acc: 0.9139\n",
      "Epoch 9/100\n",
      "30381/30381 [==============================] - 1s 29us/step - loss: 0.1937 - acc: 0.9199\n",
      "Epoch 10/100\n",
      "30381/30381 [==============================] - 1s 29us/step - loss: 0.1847 - acc: 0.9242: 0s - loss: 0.175\n",
      "Epoch 11/100\n",
      "30381/30381 [==============================] - 1s 29us/step - loss: 0.1791 - acc: 0.9257\n",
      "Epoch 12/100\n",
      "30381/30381 [==============================] - 1s 29us/step - loss: 0.1716 - acc: 0.9302\n",
      "Epoch 13/100\n",
      "30381/30381 [==============================] - 1s 29us/step - loss: 0.1666 - acc: 0.9321\n",
      "Epoch 14/100\n",
      "30381/30381 [==============================] - 1s 29us/step - loss: 0.1613 - acc: 0.9337\n",
      "Epoch 15/100\n",
      "30381/30381 [==============================] - 1s 30us/step - loss: 0.1569 - acc: 0.9375\n",
      "Epoch 16/100\n",
      "30381/30381 [==============================] - 1s 30us/step - loss: 0.1545 - acc: 0.9390\n",
      "Epoch 17/100\n",
      "30381/30381 [==============================] - 1s 28us/step - loss: 0.1464 - acc: 0.9428\n",
      "Epoch 18/100\n",
      "30381/30381 [==============================] - 1s 29us/step - loss: 0.1459 - acc: 0.9411\n",
      "Epoch 19/100\n",
      "30381/30381 [==============================] - 1s 29us/step - loss: 0.1415 - acc: 0.9456\n",
      "Epoch 20/100\n",
      "30381/30381 [==============================] - 1s 29us/step - loss: 0.1389 - acc: 0.9449\n",
      "Epoch 21/100\n",
      "30381/30381 [==============================] - 1s 29us/step - loss: 0.1333 - acc: 0.9487\n",
      "Epoch 22/100\n",
      "30381/30381 [==============================] - 1s 29us/step - loss: 0.1327 - acc: 0.9484\n",
      "Epoch 23/100\n",
      "30381/30381 [==============================] - 1s 29us/step - loss: 0.1306 - acc: 0.9488\n",
      "Epoch 24/100\n",
      "30381/30381 [==============================] - 1s 29us/step - loss: 0.1260 - acc: 0.9504\n",
      "Epoch 25/100\n",
      "30381/30381 [==============================] - 1s 29us/step - loss: 0.1232 - acc: 0.9525\n",
      "Epoch 26/100\n",
      "30381/30381 [==============================] - 1s 29us/step - loss: 0.1175 - acc: 0.9555\n",
      "Epoch 27/100\n",
      "30381/30381 [==============================] - 1s 29us/step - loss: 0.1192 - acc: 0.9534\n",
      "Epoch 28/100\n",
      "30381/30381 [==============================] - 1s 29us/step - loss: 0.1161 - acc: 0.9549\n",
      "Epoch 29/100\n",
      "30381/30381 [==============================] - 1s 29us/step - loss: 0.1131 - acc: 0.9574\n",
      "Epoch 30/100\n",
      "30381/30381 [==============================] - 1s 30us/step - loss: 0.1133 - acc: 0.9565\n",
      "Epoch 31/100\n",
      "30381/30381 [==============================] - 1s 29us/step - loss: 0.1099 - acc: 0.9579\n",
      "Epoch 32/100\n",
      "30381/30381 [==============================] - 1s 29us/step - loss: 0.1067 - acc: 0.9605\n",
      "Epoch 33/100\n",
      "30381/30381 [==============================] - 1s 29us/step - loss: 0.1068 - acc: 0.9593\n",
      "Epoch 34/100\n",
      "30381/30381 [==============================] - 1s 30us/step - loss: 0.1028 - acc: 0.9614\n",
      "Epoch 35/100\n",
      "30381/30381 [==============================] - 1s 29us/step - loss: 0.0996 - acc: 0.9626\n",
      "Epoch 36/100\n",
      "30381/30381 [==============================] - 1s 28us/step - loss: 0.1004 - acc: 0.9615\n",
      "Epoch 37/100\n",
      "30381/30381 [==============================] - 1s 28us/step - loss: 0.0987 - acc: 0.9626\n",
      "Epoch 38/100\n",
      "30381/30381 [==============================] - 1s 28us/step - loss: 0.0961 - acc: 0.9642\n",
      "Epoch 39/100\n",
      "30381/30381 [==============================] - 1s 29us/step - loss: 0.0936 - acc: 0.9653\n",
      "Epoch 40/100\n",
      "30381/30381 [==============================] - 1s 28us/step - loss: 0.0918 - acc: 0.9655\n",
      "Epoch 41/100\n",
      "30381/30381 [==============================] - 1s 29us/step - loss: 0.0933 - acc: 0.9652\n",
      "Epoch 42/100\n",
      "30381/30381 [==============================] - 1s 28us/step - loss: 0.0892 - acc: 0.9657\n",
      "Epoch 43/100\n",
      "30381/30381 [==============================] - 1s 28us/step - loss: 0.0887 - acc: 0.9662\n",
      "Epoch 44/100\n",
      "30381/30381 [==============================] - 1s 28us/step - loss: 0.0861 - acc: 0.9684\n",
      "Epoch 45/100\n",
      "30381/30381 [==============================] - 1s 29us/step - loss: 0.0846 - acc: 0.9675\n",
      "Epoch 46/100\n",
      "30381/30381 [==============================] - 1s 29us/step - loss: 0.0834 - acc: 0.9688\n",
      "Epoch 47/100\n",
      "30381/30381 [==============================] - 1s 29us/step - loss: 0.0823 - acc: 0.9697\n",
      "Epoch 48/100\n",
      "30381/30381 [==============================] - 1s 29us/step - loss: 0.0830 - acc: 0.9688\n",
      "Epoch 49/100\n",
      "30381/30381 [==============================] - 1s 29us/step - loss: 0.0793 - acc: 0.9701\n",
      "Epoch 50/100\n",
      "30381/30381 [==============================] - 1s 29us/step - loss: 0.0799 - acc: 0.9697\n",
      "Epoch 51/100\n",
      "30381/30381 [==============================] - 1s 29us/step - loss: 0.0751 - acc: 0.9718\n",
      "Epoch 52/100\n",
      "30381/30381 [==============================] - 1s 31us/step - loss: 0.0781 - acc: 0.9703\n",
      "Epoch 53/100\n",
      "30381/30381 [==============================] - 1s 28us/step - loss: 0.0786 - acc: 0.9703\n",
      "Epoch 54/100\n",
      "30381/30381 [==============================] - 1s 28us/step - loss: 0.0732 - acc: 0.9721\n",
      "Epoch 55/100\n",
      "30381/30381 [==============================] - 1s 28us/step - loss: 0.0712 - acc: 0.9736\n",
      "Epoch 56/100\n",
      "30381/30381 [==============================] - 1s 28us/step - loss: 0.0721 - acc: 0.9734\n",
      "Epoch 57/100\n",
      "30381/30381 [==============================] - 1s 29us/step - loss: 0.0717 - acc: 0.9730\n",
      "Epoch 58/100\n",
      "30381/30381 [==============================] - 1s 28us/step - loss: 0.0695 - acc: 0.9749\n",
      "Epoch 59/100\n",
      "30381/30381 [==============================] - 1s 29us/step - loss: 0.0672 - acc: 0.9749\n",
      "Epoch 60/100\n",
      "30381/30381 [==============================] - 1s 29us/step - loss: 0.0676 - acc: 0.9751\n",
      "Epoch 61/100\n",
      "30381/30381 [==============================] - 1s 29us/step - loss: 0.0645 - acc: 0.9767\n",
      "Epoch 62/100\n",
      "30381/30381 [==============================] - 1s 29us/step - loss: 0.0668 - acc: 0.9751\n",
      "Epoch 63/100\n",
      "30381/30381 [==============================] - 1s 29us/step - loss: 0.0650 - acc: 0.9762\n",
      "Epoch 64/100\n",
      "30381/30381 [==============================] - 1s 29us/step - loss: 0.0612 - acc: 0.9772\n",
      "Epoch 65/100\n",
      "30381/30381 [==============================] - 1s 29us/step - loss: 0.0616 - acc: 0.9763\n",
      "Epoch 66/100\n",
      "30381/30381 [==============================] - 1s 29us/step - loss: 0.0594 - acc: 0.9781\n",
      "Epoch 67/100\n",
      "30381/30381 [==============================] - 1s 29us/step - loss: 0.0638 - acc: 0.9759\n",
      "Epoch 68/100\n",
      "30381/30381 [==============================] - 1s 29us/step - loss: 0.0589 - acc: 0.9784\n",
      "Epoch 69/100\n",
      "30381/30381 [==============================] - 1s 29us/step - loss: 0.0586 - acc: 0.9782\n",
      "Epoch 70/100\n",
      "30381/30381 [==============================] - 1s 30us/step - loss: 0.0543 - acc: 0.9805\n",
      "Epoch 71/100\n",
      "30381/30381 [==============================] - 1s 29us/step - loss: 0.0542 - acc: 0.9797\n",
      "Epoch 72/100\n",
      "30381/30381 [==============================] - 1s 28us/step - loss: 0.0564 - acc: 0.9789\n",
      "Epoch 73/100\n",
      "30381/30381 [==============================] - 1s 28us/step - loss: 0.0558 - acc: 0.9790\n",
      "Epoch 74/100\n",
      "30381/30381 [==============================] - 1s 29us/step - loss: 0.0507 - acc: 0.9812\n",
      "Epoch 75/100\n",
      "30381/30381 [==============================] - 1s 29us/step - loss: 0.0534 - acc: 0.9797\n",
      "Epoch 76/100\n",
      "30381/30381 [==============================] - 1s 28us/step - loss: 0.0539 - acc: 0.9797\n",
      "Epoch 77/100\n",
      "30381/30381 [==============================] - 1s 28us/step - loss: 0.0477 - acc: 0.9833\n",
      "Epoch 78/100\n",
      "30381/30381 [==============================] - 1s 28us/step - loss: 0.0513 - acc: 0.9811\n",
      "Epoch 79/100\n",
      "30381/30381 [==============================] - 1s 28us/step - loss: 0.0482 - acc: 0.9828\n",
      "Epoch 80/100\n",
      "30381/30381 [==============================] - 1s 28us/step - loss: 0.0546 - acc: 0.9797\n",
      "Epoch 81/100\n",
      "30381/30381 [==============================] - 1s 28us/step - loss: 0.0477 - acc: 0.9820\n",
      "Epoch 82/100\n",
      "30381/30381 [==============================] - 1s 28us/step - loss: 0.0464 - acc: 0.9832\n",
      "Epoch 83/100\n",
      "30381/30381 [==============================] - 1s 28us/step - loss: 0.0471 - acc: 0.9829\n",
      "Epoch 84/100\n",
      "30381/30381 [==============================] - 1s 28us/step - loss: 0.0508 - acc: 0.9812\n",
      "Epoch 85/100\n",
      "30381/30381 [==============================] - 1s 28us/step - loss: 0.0436 - acc: 0.9843\n",
      "Epoch 86/100\n",
      "30381/30381 [==============================] - 1s 29us/step - loss: 0.0482 - acc: 0.9818\n",
      "Epoch 87/100\n",
      "30381/30381 [==============================] - 1s 29us/step - loss: 0.0412 - acc: 0.9850\n",
      "Epoch 88/100\n",
      "30381/30381 [==============================] - 1s 29us/step - loss: 0.0447 - acc: 0.9838\n",
      "Epoch 89/100\n",
      "30381/30381 [==============================] - 1s 31us/step - loss: 0.0407 - acc: 0.9850\n",
      "Epoch 90/100\n",
      "30381/30381 [==============================] - 1s 28us/step - loss: 0.0444 - acc: 0.9836\n",
      "Epoch 91/100\n",
      "30381/30381 [==============================] - 1s 29us/step - loss: 0.0417 - acc: 0.9845\n",
      "Epoch 92/100\n",
      "30381/30381 [==============================] - 1s 28us/step - loss: 0.0411 - acc: 0.9855\n",
      "Epoch 93/100\n",
      "30381/30381 [==============================] - 1s 29us/step - loss: 0.0423 - acc: 0.9843\n",
      "Epoch 94/100\n",
      "30381/30381 [==============================] - 1s 29us/step - loss: 0.0407 - acc: 0.9856\n",
      "Epoch 95/100\n",
      "30381/30381 [==============================] - 1s 29us/step - loss: 0.0404 - acc: 0.9854\n",
      "Epoch 96/100\n",
      "30381/30381 [==============================] - 1s 28us/step - loss: 0.0362 - acc: 0.9874\n",
      "Epoch 97/100\n",
      "30381/30381 [==============================] - 1s 29us/step - loss: 0.0387 - acc: 0.9858\n",
      "Epoch 98/100\n",
      "30381/30381 [==============================] - 1s 29us/step - loss: 0.0427 - acc: 0.9838\n",
      "Epoch 99/100\n",
      "30381/30381 [==============================] - 1s 29us/step - loss: 0.0344 - acc: 0.9883\n",
      "Epoch 100/100\n",
      "30381/30381 [==============================] - 1s 29us/step - loss: 0.0437 - acc: 0.9835\n",
      "1999 1.0 1.0 0.62\n",
      "3999 0.99 1.0 0.59\n",
      "5999 0.98 1.0 0.68\n",
      "7999 1.0 1.0 0.64\n",
      "9999 0.95 1.0 0.66\n",
      "train re-gan\n",
      "500/500 [==============================] - 0s 427us/step\n",
      "1999 0.31729747843742373\n",
      "500/500 [==============================] - 0s 20us/step\n",
      "3999 0.31058266854286193\n",
      "500/500 [==============================] - 0s 16us/step\n",
      "5999 0.3136694808006287\n",
      "500/500 [==============================] - 0s 19us/step\n",
      "7999 0.30643029618263246\n",
      "500/500 [==============================] - 0s 16us/step\n",
      "9999 0.31165598487854\n",
      "test...........\n",
      "[[  5.494978    -1.8686646   -0.08362104  -1.1248827   -1.6275195\n",
      "   -2.7571766    0.1038942  -14.751401    -3.8230615   -4.6931443\n",
      "   -2.7269795   -1.0884105    3.0042062    0.7453372   -1.1093271\n",
      "    0.5316773   -5.0382895   -5.0598655    2.2317402   -4.299391  ]\n",
      " [  0.9478212   -1.4322827    0.87529075  -0.8640615   -2.1414316\n",
      "   -1.6033189   -1.5010042   -6.6947308   -1.3429631   -2.3010135\n",
      "   -1.8179386   -0.40125692   2.2130961   -0.34524676  -0.6390381\n",
      "    3.0986686   -0.53366137  -2.0947914    1.411536    -2.3353956 ]\n",
      " [  2.8747628   -1.9357226    1.5147007   -1.570246    -2.0818157\n",
      "   -1.5117457   -0.6701251   -7.9104147   -0.65290165  -0.7022064\n",
      "   -3.03002      0.289857     2.498237     0.10348576  -1.9523166\n",
      "    0.594887    -2.9186893   -1.5586815   -0.1263718   -3.0275156 ]]\n",
      "test2...........\n",
      "[[0.         0.47727273 0.         0.2        0.33333333 0.33333333\n",
      "  0.2        0.         0.51162791 0.4        0.5        0.83333333\n",
      "  0.         0.         0.18276571 0.46067416 0.24523052 0.24\n",
      "  0.28829916 0.        ]\n",
      " [1.         0.72727273 0.         0.2        0.83333333 1.\n",
      "  0.2        0.         0.60465116 0.8        0.16666667 0.16666667\n",
      "  0.12894907 0.         0.28288315 0.63958513 0.58386328 0.504\n",
      "  0.45597105 0.        ]\n",
      " [1.         0.36363636 1.         0.6        0.83333333 0.33333333\n",
      "  0.         0.         0.34883721 0.2        0.5        0.16666667\n",
      "  0.87298517 0.52965028 0.82897827 0.27484875 0.97436407 0.92\n",
      "  0.49698432 0.07892004]]\n",
      "[[ 0.05353669 -0.01757337 -0.02654589 -0.01234739 -0.02316859 -0.03509521\n",
      "  -0.0123328  -0.15823393 -0.01814371 -0.01045616 -0.03030646 -0.01230332\n",
      "   0.02793413 -0.01457155 -0.05619732  0.00505677 -0.05051126 -0.03788763\n",
      "   0.02144984 -0.03903876]\n",
      " [ 0.1210989  -0.01769364 -0.04415711  0.00816699 -0.02625421 -0.03582795\n",
      "   0.01534036 -0.2322268   0.00417057  0.01100608 -0.06178496 -0.05478502\n",
      "   0.05256132 -0.02342492 -0.06149753 -0.0178639  -0.08079748 -0.05273511\n",
      "   0.03996073 -0.04722214]\n",
      " [ 0.16928664 -0.00884513  0.04901439  0.01197308 -0.0661551  -0.07157054\n",
      "  -0.05227295 -0.37609076 -0.01679006 -0.09894919 -0.05353876 -0.03462862\n",
      "   0.16421823  0.00183078 -0.00947469  0.07522419 -0.11372858 -0.09020056\n",
      "   0.14394431 -0.15278691]]\n",
      "--------\n",
      "test3...........\n",
      "0.0 : 2.1634555924126335\n",
      "0.2 : 2.1626289360330713\n",
      "0.4 : 2.1818995538722055\n",
      "0.6 : 2.219318882416946\n",
      "0.8 : 2.2737149055311656\n",
      "1.0 : 2.3438992190033323\n",
      "win: 1.0\n",
      "0.0 : 2.2023591231379895\n",
      "0.2 : 2.209506985256539\n",
      "0.4 : 2.2342672833741233\n",
      "0.6 : 2.2762884587960004\n",
      "0.8 : 2.334492547149541\n",
      "1.0 : 2.4077852230927475\n",
      "win: 0.4\n"
     ]
    }
   ],
   "source": [
    "# train a generative adversarial network on a one-dimensional function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from numpy import hstack\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy.random import rand\n",
    "from numpy.random import randn\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from matplotlib import pyplot\n",
    "import random \n",
    "import math\n",
    "\n",
    "    \n",
    "def prepare_data():\n",
    "\n",
    "    rdata0 = pd.read_csv(\"BankChurners.csv\")\n",
    "    rdata = rdata0.drop([rdata0.columns[0],rdata0.columns[-1],rdata0.columns[-2]],axis=1)\n",
    "    rdata.head()\n",
    "\n",
    "    Dicts = []\n",
    "    for c in range(len(rdata.columns)):\n",
    "        l1 = rdata[rdata.columns[c]]\n",
    "        l2 = list(dict.fromkeys(l1))\n",
    "        l2.sort()\n",
    "        N = len(l2)-1\n",
    "        Dicts.append(dict())\n",
    "        for i in range(len(l2)):\n",
    "            Dicts[c] [l2[i]] = i / N \n",
    "\n",
    "\n",
    "    for c in range(len(rdata.columns)):\n",
    "        print (rdata.columns[c],':', len(Dicts[c]))\n",
    "    #    print (\" \")\n",
    "\n",
    "\n",
    "    print (rdata.shape)\n",
    "\n",
    "    DArray = np.zeros(  (rdata.shape[0], rdata.shape[1]),dtype = \"float\")\n",
    "    print (DArray.shape)\n",
    "\n",
    "    for n in range(rdata.shape[0]):\n",
    "        for c in range(rdata.shape[1]):\n",
    "            #print (rdata [n,c])\n",
    "            DArray[n,c] =  Dicts[c] [ rdata.iat [n,c]]\n",
    "\n",
    "    #print (DArray[1:20,3:6])\n",
    "    CCnt = []\n",
    "    for c in range(len(rdata.columns)):\n",
    "        CCnt.append(len(Dicts[c]))\n",
    "    return (DArray,rdata.columns,CCnt)\n",
    "    \n",
    "    \n",
    "def train_discrimantor(discriminator,N1=30000):\n",
    "\n",
    "\n",
    "    N = DArray.shape[0]\n",
    "    \n",
    "    N1 = min(N1,N)\n",
    "    \n",
    "    x_real = np.zeros((N1,data_dim))\n",
    "    y_real = np.zeros((N1,1))\n",
    "\n",
    "    sampl = random.sample(range(N),N1)\n",
    "    \n",
    "    for n in range(N1):\n",
    "        for j in range(data_dim):\n",
    "            x_real[n,j] = DArray[sampl[n]][j] \n",
    "        y_real[n,0] = 1\n",
    "\n",
    "\n",
    "    x_fake = np.zeros((N1,data_dim))\n",
    "    y_fake = np.zeros((N1,1))\n",
    "\n",
    "    for n in range(N1):\n",
    "        idx = random.sample(range(data_dim),8)\n",
    "        for j in range(data_dim):\n",
    "            x_fake[n,j] = x_real[n,j]\n",
    "        for j in idx:\n",
    "            x_fake[n,j] += (random.random() - .5)*.6\n",
    "            x_fake[n,j] = min(1, max(x_fake[n,j],0))\n",
    "        y_fake[n,0] = 0\n",
    "\n",
    "    x_train = np.zeros((3*N1, data_dim))\n",
    "    y_train = np.zeros((3*N1, 1))\n",
    "    for n in range(N1):\n",
    "        for j in range(data_dim):\n",
    "            x_train[3*n,j] = x_real[n,j]\n",
    "            x_train[3*n+1,j] = x_fake[n,j]\n",
    "            x_train[3*n+2,j] = random.random()\n",
    "        y_train [3*n,0] = 1\n",
    "        y_train [3*n + 1,0] = 0\n",
    "        y_train [3*n + 2,0] = 0\n",
    "        \n",
    "    \n",
    "\n",
    "    discriminator.fit(x_train, y_train,epochs=100)    \n",
    "\n",
    "    \n",
    "# define the standalone discriminator model\n",
    "def define_discriminator(n_inputs):\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(8 *n_inputs, activation='relu', kernel_initializer='he_uniform', input_dim=n_inputs))\n",
    "\tmodel.add(Dense(8 *n_inputs, activation='relu', kernel_initializer='he_uniform', input_dim=n_inputs))\n",
    "\tmodel.add(Dense(1, activation='sigmoid'))\n",
    "\t# compile model\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model\n",
    " \n",
    "# define the standalone generator model\n",
    "def define_generator(latent_dim, n_outputs):\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(6*(latent_dim +  n_outputs), activation='relu', kernel_initializer='he_uniform', input_dim=latent_dim))\n",
    "\tmodel.add(Dense(6*(latent_dim +  n_outputs), activation='relu', kernel_initializer='he_uniform', input_dim=latent_dim))\n",
    "\tmodel.add(Dense(n_outputs, activation='linear'))\n",
    "\treturn model\n",
    "\n",
    "# define the standalone generator model\n",
    "def define_regenerator(latent_dim, n_outputs):\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(6*(latent_dim +  n_outputs), activation='relu', kernel_initializer='he_uniform', input_dim=latent_dim))\n",
    "\tmodel.add(Dense(6*(latent_dim +  n_outputs), activation='relu', kernel_initializer='he_uniform', input_dim=latent_dim))\n",
    "\tmodel.add(Dense(n_outputs, activation='linear'))\n",
    "\treturn model\n",
    "\n",
    "\n",
    "def define_regen(generator, regenerator):\n",
    "\t# make weights in the discriminator not trainable\n",
    "\tgenerator.trainable = False\n",
    "\t# connect them\n",
    "\tmodel = Sequential()\n",
    "\t# add generator\n",
    "\tmodel.add(regenerator)\n",
    "\t# add the discriminator\n",
    "\tmodel.add(generator)\n",
    "\t# compile model\n",
    "\tmodel.compile(loss='mean_squared_error', optimizer='adam',metrics=['mse'])\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# define the combined generator and discriminator model, for updating the generator\n",
    "def define_gan(generator, discriminator):\n",
    "\t# make weights in the discriminator not trainable\n",
    "\tdiscriminator.trainable = False\n",
    "\t# connect them\n",
    "\tmodel = Sequential()\n",
    "\t# add generator\n",
    "\tmodel.add(generator)\n",
    "\t# add the discriminator\n",
    "\tmodel.add(discriminator)\n",
    "\t# compile model\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\treturn model\n",
    " \n",
    "# generate n real samples with class labels\n",
    "def generate_real_samples(DArray, n):\n",
    "    \n",
    "    X = np.zeros((n,DArray.shape[1]))\n",
    "    indx = random.sample(range(DArray.shape[0]),n)\n",
    "    \n",
    "    for i in range(len(indx)):\n",
    "        for j in range(DArray.shape[1]):\n",
    "            X[i,j] = DArray[indx[i],j]\n",
    "    # generate class labels\n",
    "    y = ones((n, 1))\n",
    "    return X, y\n",
    " \n",
    "# generate points in latent space as input for the generator\n",
    "def generate_latent_points(latent_dim, n):\n",
    "\t# generate points in the latent space\n",
    "\tx_input = randn(latent_dim * n)\n",
    "\t# reshape into a batch of inputs for the network\n",
    "\tx_input = x_input.reshape(n, latent_dim)\n",
    "\treturn x_input\n",
    " \n",
    "# use the generator to generate n fake examples, with class labels\n",
    "def generate_fake_samples(generator, latent_dim, n):\n",
    "\t# generate points in latent space\n",
    "\tx_input = generate_latent_points(latent_dim, n)\n",
    "\t# predict outputs\n",
    "\tX = generator.predict(x_input)\n",
    "\t# create class labels\n",
    "\ty = zeros((n, 1))\n",
    "\treturn X, y\n",
    " \n",
    "# evaluate the discriminator and plot real and fake points\n",
    "def summarize_performance(DArray, epoch, generator, discriminator, latent_dim, n=100):\n",
    "\t# prepare real samples\n",
    "\tx_real, y_real = generate_real_samples(DArray, n)\n",
    "\t# evaluate discriminator on real examples\n",
    "\t_, acc_real = discriminator.evaluate(x_real, y_real, verbose=0)\n",
    "\t# prepare fake examples\n",
    "\tx_gen, y_gen = generate_fake_samples(generator, latent_dim, n)\n",
    "\ty_gen = np.ones((n,1))\n",
    "\t# evaluate discriminator on fake examples\n",
    "\t_, acc_gen = discriminator.evaluate(x_gen, y_gen, verbose=0)\n",
    "    # random     \n",
    "\tx_rnd = randn(DArray.shape[1] * n)\n",
    "\tx_rnd = x_rnd.reshape(n, DArray.shape[1])    \n",
    "\ty_rnd = np.zeros((n,1))\n",
    "\t_, acc_rnd = discriminator.evaluate(x_rnd, y_rnd, verbose=0)\n",
    "    \n",
    "\t# summarize discriminator performance\n",
    "\tprint(epoch, acc_real, acc_gen,acc_rnd)\n",
    " \n",
    "# train the generator and discriminator\n",
    "def train_regen(DArray, regen_model,  n_epochs, n_batch, n_eval):\n",
    "    # determine half the size of one batch, for updating the discriminator\n",
    "    half_batch = int(n_batch / 2)\n",
    "    # manually enumerate epochs\n",
    "    for i in range(n_epochs):\n",
    "        # prepare real samples\n",
    "        x_real, _ = generate_real_samples(DArray, half_batch)\n",
    "        y_real = x_real.copy()\n",
    "        \n",
    "        regen_model.train_on_batch(x_real, y_real)\n",
    "        # evaluate the model every n_eval epochs\n",
    "        if (i+1) % n_eval == 0:\n",
    "            _ , mse = regen_model.evaluate(x_real, y_real)\n",
    "            print (i, mse)\n",
    " \n",
    "# train the generator and discriminator\n",
    "def train(DArray, g_model, d_model, gan_model, latent_dim, n_epochs, n_batch, n_eval):\n",
    "\t# determine half the size of one batch, for updating the discriminator\n",
    "\thalf_batch = int(n_batch / 2)\n",
    "\t# manually enumerate epochs\n",
    "\tfor i in range(n_epochs):\n",
    "\t\t# prepare real samples\n",
    "\t\tx_real, y_real = generate_real_samples(DArray, half_batch)\n",
    "\t\t# prepare fake examples\n",
    "\t\tx_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
    "\t\t# update discriminator\n",
    "\t\td_model.train_on_batch(x_real, y_real)\n",
    "\t\td_model.train_on_batch(x_fake, y_fake)\n",
    "\t\t# prepare points in latent space as input for the generator\n",
    "\t\tx_gan = generate_latent_points(latent_dim, n_batch)\n",
    "\t\t# create inverted labels for the fake samples\n",
    "\t\ty_gan = ones((n_batch, 1))\n",
    "\t\t# update the generator via the discriminator's error\n",
    "\t\tgan_model.train_on_batch(x_gan, y_gan)\n",
    "\t\t# evaluate the model every n_eval epochs\n",
    "\t\tif (i+1) % n_eval == 0:\n",
    "\t\t\tsummarize_performance(DArray, i, g_model, d_model, latent_dim)\n",
    "\n",
    "            \n",
    "def train_2(DArray, g_model, d_model, gan_model, latent_dim, n_epochs, n_batch, n_eval):\n",
    "\t# determine half the size of one batch, for updating the discriminator\n",
    "\thalf_batch = int(n_batch / 2)\n",
    "\t# manually enumerate epochs\n",
    "\tfor i in range(n_epochs):\n",
    "\t\t# prepare points in latent space as input for the generator\n",
    "\t\tx_gan = generate_latent_points(latent_dim, n_batch)\n",
    "\t\t# create inverted labels for the fake samples\n",
    "\t\ty_gan = ones((n_batch, 1))\n",
    "\t\t# update the generator via the discriminator's error\n",
    "\t\tgan_model.train_on_batch(x_gan, y_gan)\n",
    "\t\t# evaluate the model every n_eval epochs\n",
    "\t\tif (i+1) % n_eval == 0:\n",
    "\t\t\tsummarize_performance(DArray, i, g_model, d_model, latent_dim)\n",
    "            \n",
    "            \n",
    "def distance(X1,X2,ldim):\n",
    "    d = 0\n",
    "    for i in range(ldim):\n",
    "        d = d + (X1[i]-X2[i])**2\n",
    "    return math.sqrt(d)\n",
    "\n",
    "\n",
    "(DArray, cnames, CCnt) = prepare_data()\n",
    "\n",
    "print (\"shape\",DArray.shape)\n",
    "latent_dim = 12  # 10\n",
    "data_dim = DArray.shape[1]\n",
    "# create the discriminator\n",
    "discriminator = define_discriminator(data_dim)\n",
    "# create the generator\n",
    "generator = define_generator(latent_dim,data_dim)\n",
    "# create the gan\n",
    "gan_model = define_gan(generator, discriminator)\n",
    "# train model\n",
    "n_epochs=10000\n",
    "n_eval=2000\n",
    "n_batch=1000\n",
    "print (\"train base gan\")\n",
    "\n",
    "train_discrimantor(discriminator)\n",
    "\n",
    "#train(DArray, generator, discriminator, gan_model, latent_dim,n_epochs,n_batch, n_eval)\n",
    "train_2(DArray, generator, discriminator, gan_model, latent_dim,n_epochs,n_batch, n_eval)\n",
    "\n",
    "regenerator =  define_regenerator(data_dim, latent_dim) \n",
    "regen = define_regen(generator, regenerator)\n",
    "#regen.summary()\n",
    "print (\"train re-gan\")\n",
    "train_regen(DArray, regen, n_epochs, n_batch, n_eval)\n",
    "\n",
    "\n",
    "# test data\n",
    "print (\"test...........\")\n",
    "n_batch=3\n",
    "\n",
    "x_gan = generate_latent_points(latent_dim, n_batch)\n",
    "y_pred = generator.predict(x_gan)\n",
    "print (y_pred)\n",
    "\n",
    "\n",
    "print (\"test2...........\")\n",
    "n_batch=3\n",
    "\n",
    "x_real, _ = generate_real_samples(DArray, n_batch)\n",
    "y_pred = regen.predict(x_real)\n",
    "print (x_real)\n",
    "print (y_pred)\n",
    "print(\"--------\")\n",
    "\n",
    "\n",
    "print (\"test3...........\")\n",
    "Qdim = 3\n",
    "x_test = np.zeros((CCnt[Qdim],data_dim))\n",
    "for i in [1235,5621]:\n",
    "    win = DArray[i,Qdim]\n",
    "    for n in range(CCnt[Qdim]):\n",
    "        c = 0\n",
    "        for j in range(data_dim):\n",
    "            if j != Qdim:\n",
    "                x_test[n,j] = DArray[i,j]\n",
    "            else:\n",
    "                x_test[n,j] = n/(CCnt[Qdim]-1)\n",
    "    \n",
    "    y_pred = regen.predict(x_test)\n",
    "    \n",
    "    for n in range(CCnt[Qdim]):\n",
    "        d  = distance(list(y_pred[n,:]), list(x_test[n,:]),data_dim)\n",
    "        print (x_test[n,Qdim],\":\",d)\n",
    "    print (\"win:\", win)\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test3...........\n",
      "0.0 : 0.6465046698361586\n",
      "0.2 : 0.5914369057209591\n",
      "0.4 : 0.577098298766065\n",
      "0.6 : 0.5848754720765437\n",
      "0.8 : 0.6249829900204374\n",
      "1.0 : 0.6930064919626696\n",
      "win: 0.6\n"
     ]
    }
   ],
   "source": [
    "print (\"test3...........\")\n",
    "\n",
    "Qdim = 3\n",
    "x_test = np.zeros((CCnt[Qdim],data_dim))\n",
    "for i in [835]:\n",
    "    win = DArray[i,Qdim]\n",
    "    for n in range(CCnt[Qdim]):\n",
    "        c = 0\n",
    "        for j in range(data_dim):\n",
    "            if j != Qdim:\n",
    "                x_test[n,j] = DArray[i,j]\n",
    "            else:\n",
    "                x_test[n,j] = n/(CCnt[Qdim]-1)\n",
    "    \n",
    "    y_pred = regen.predict(x_test)\n",
    "    \n",
    "    for n in range(CCnt[Qdim]):\n",
    "        d  = distance(list(y_pred[n,:]), list(x_test[n,:]),data_dim)\n",
    "        print (x_test[n,Qdim],\":\",d)\n",
    "    print (\"win:\", win)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.61363636 0.         0.4        1.         0.66666667\n",
      "  0.8        0.         0.53488372 1.         0.33333333 0.16666667\n",
      "  0.01273372 0.29498226 0.04888432 0.55661193 0.55743243 0.592\n",
      "  0.40289505 0.69781931]]\n",
      "[[0.98917866]]\n",
      "[[0.53019211 0.3240497  0.55923963 0.94599776 0.51249114 0.65014849\n",
      "  0.35376479 0.02413595 0.27074096 0.62433232 0.73235009 0.22553657\n",
      "  0.18585281 0.99392715 0.78029393 0.7104573  0.18822425 0.78631476\n",
      "  0.03678716 0.64022728]]\n",
      "[[0.]]\n",
      "[[1.0000000e+00 5.1363636e-01 0.0000000e+00 8.0000000e-01 3.0000000e-04\n",
      "  1.6666667e-01 0.0000000e+00 0.0000000e+00 5.3488372e-01 1.0000000e+00\n",
      "  1.3333333e-01 1.6666667e-01 1.2733720e-02 2.9498226e-01 4.8884320e-02\n",
      "  1.5661193e-01 5.5743243e-01 5.9200000e-01 4.0289505e-01 6.9781931e-01]]\n",
      "[[0.99823654]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x = np.zeros((1,data_dim))\n",
    "for j  in range(data_dim):\n",
    "    x[0,j] = DArray[6456,j]\n",
    "print (x)\n",
    "y = discriminator.predict(x)\n",
    "print (y)\n",
    "\n",
    "x = np.zeros((1,data_dim))\n",
    "for j  in range(data_dim):\n",
    "    x[0,j] = random.random()\n",
    "print (x)\n",
    "y = discriminator.predict(x)\n",
    "print (y)\n",
    "\n",
    "z = [1.,         0.61363636, 0.,         0.4,        1.,         0.66666667,\n",
    "  0.8,        0.,         0.53488372, 1.,         0.33333333, 0.16666667,\n",
    "  0.01273372, 0.29498226, 0.04888432, 0.55661193, 0.55743243, 0.592,\n",
    "  0.40289505, 0.69781931]\n",
    "z = [1.,         0.51363636, 0.,         0.8,        0.0003,         0.16666667,\n",
    "  0.0,        0.,         0.53488372, 1.,         0.13333333, 0.16666667,\n",
    "  0.01273372, 0.29498226, 0.04888432, 0.15661193, 0.55743243, 0.592,\n",
    "  0.40289505, 0.69781931]\n",
    "x = np.zeros((1,data_dim))\n",
    "for j  in range(data_dim):\n",
    "    x[0,j] = z[j]\n",
    "print (x)\n",
    "y = discriminator.predict(x)\n",
    "print (y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attrition_Flag : 2\n",
      "Customer_Age : 45\n",
      "Gender : 2\n",
      "Dependent_count : 6\n",
      "Education_Level : 7\n",
      "Marital_Status : 4\n",
      "Income_Category : 6\n",
      "Card_Category : 4\n",
      "Months_on_book : 44\n",
      "Total_Relationship_Count : 6\n",
      "Months_Inactive_12_mon : 7\n",
      "Contacts_Count_12_mon : 7\n",
      "Credit_Limit : 6205\n",
      "Total_Revolving_Bal : 1974\n",
      "Avg_Open_To_Buy : 6813\n",
      "Total_Amt_Chng_Q4_Q1 : 1158\n",
      "Total_Trans_Amt : 5033\n",
      "Total_Trans_Ct : 126\n",
      "Total_Ct_Chng_Q4_Q1 : 830\n",
      "Avg_Utilization_Ratio : 964\n",
      "(10127, 20)\n",
      "(10127, 20)\n",
      "Attribute  Dependent_count dc= 0.2\n",
      "Epoch 1/100\n",
      "8101/8101 [==============================] - 1s 169us/step - loss: 1.3095 - acc: 0.5283\n",
      "Epoch 2/100\n",
      "8101/8101 [==============================] - 0s 28us/step - loss: 1.2821 - acc: 0.5323\n",
      "Epoch 3/100\n",
      "8101/8101 [==============================] - 0s 28us/step - loss: 1.2776 - acc: 0.5323\n",
      "Epoch 4/100\n",
      "8101/8101 [==============================] - 0s 28us/step - loss: 1.2749 - acc: 0.5323\n",
      "Epoch 5/100\n",
      "8101/8101 [==============================] - 0s 27us/step - loss: 1.2713 - acc: 0.5323\n",
      "Epoch 6/100\n",
      "8101/8101 [==============================] - 0s 28us/step - loss: 1.2689 - acc: 0.5324\n",
      "Epoch 7/100\n",
      "8101/8101 [==============================] - 0s 28us/step - loss: 1.2662 - acc: 0.5323\n",
      "Epoch 8/100\n",
      "8101/8101 [==============================] - 0s 28us/step - loss: 1.2649 - acc: 0.5325\n",
      "Epoch 9/100\n",
      "8101/8101 [==============================] - 0s 28us/step - loss: 1.2637 - acc: 0.5325\n",
      "Epoch 10/100\n",
      "8101/8101 [==============================] - 0s 28us/step - loss: 1.2626 - acc: 0.5327\n",
      "Epoch 11/100\n",
      "8101/8101 [==============================] - 0s 28us/step - loss: 1.2623 - acc: 0.5327\n",
      "Epoch 12/100\n",
      "8101/8101 [==============================] - 0s 28us/step - loss: 1.2618 - acc: 0.5331\n",
      "Epoch 13/100\n",
      "8101/8101 [==============================] - 0s 28us/step - loss: 1.2622 - acc: 0.5340\n",
      "Epoch 14/100\n",
      "8101/8101 [==============================] - 0s 28us/step - loss: 1.2599 - acc: 0.5334\n",
      "Epoch 15/100\n",
      "8101/8101 [==============================] - 0s 27us/step - loss: 1.2622 - acc: 0.5334\n",
      "Epoch 16/100\n",
      "8101/8101 [==============================] - 0s 27us/step - loss: 1.2590 - acc: 0.5335\n",
      "Epoch 17/100\n",
      "8101/8101 [==============================] - 0s 28us/step - loss: 1.2600 - acc: 0.5339\n",
      "Epoch 18/100\n",
      "8101/8101 [==============================] - 0s 28us/step - loss: 1.2612 - acc: 0.5338\n",
      "Epoch 19/100\n",
      "8101/8101 [==============================] - 0s 27us/step - loss: 1.2587 - acc: 0.5334\n",
      "Epoch 20/100\n",
      "8101/8101 [==============================] - 0s 28us/step - loss: 1.2598 - acc: 0.5335\n",
      "Epoch 21/100\n",
      "8101/8101 [==============================] - 0s 28us/step - loss: 1.2586 - acc: 0.5338\n",
      "Epoch 22/100\n",
      "8101/8101 [==============================] - 0s 27us/step - loss: 1.2583 - acc: 0.5335\n",
      "Epoch 23/100\n",
      "8101/8101 [==============================] - 0s 27us/step - loss: 1.2585 - acc: 0.5338\n",
      "Epoch 24/100\n",
      "8101/8101 [==============================] - 0s 28us/step - loss: 1.2586 - acc: 0.5339\n",
      "Epoch 25/100\n",
      "8101/8101 [==============================] - 0s 29us/step - loss: 1.2571 - acc: 0.5343\n",
      "Epoch 26/100\n",
      "8101/8101 [==============================] - 0s 28us/step - loss: 1.2580 - acc: 0.5330\n",
      "Epoch 27/100\n",
      "8101/8101 [==============================] - 0s 28us/step - loss: 1.2571 - acc: 0.5349\n",
      "Epoch 28/100\n",
      "8101/8101 [==============================] - 0s 28us/step - loss: 1.2585 - acc: 0.5345\n",
      "Epoch 29/100\n",
      "8101/8101 [==============================] - 0s 27us/step - loss: 1.2568 - acc: 0.5340\n",
      "Epoch 30/100\n",
      "8101/8101 [==============================] - 0s 28us/step - loss: 1.2563 - acc: 0.5352\n",
      "Epoch 31/100\n",
      "8101/8101 [==============================] - 0s 27us/step - loss: 1.2549 - acc: 0.5344\n",
      "Epoch 32/100\n",
      "8101/8101 [==============================] - 0s 28us/step - loss: 1.2553 - acc: 0.5343\n",
      "Epoch 33/100\n",
      "8101/8101 [==============================] - 0s 28us/step - loss: 1.2567 - acc: 0.5345\n",
      "Epoch 34/100\n",
      "8101/8101 [==============================] - 0s 28us/step - loss: 1.2532 - acc: 0.5345\n",
      "Epoch 35/100\n",
      "8101/8101 [==============================] - 0s 27us/step - loss: 1.2513 - acc: 0.5343\n",
      "Epoch 36/100\n",
      "8101/8101 [==============================] - 0s 28us/step - loss: 1.2498 - acc: 0.5347\n",
      "Epoch 37/100\n",
      "8101/8101 [==============================] - 0s 27us/step - loss: 1.2472 - acc: 0.5341\n",
      "Epoch 38/100\n",
      "8101/8101 [==============================] - 0s 27us/step - loss: 1.2470 - acc: 0.5354\n",
      "Epoch 39/100\n",
      "8101/8101 [==============================] - 0s 28us/step - loss: 1.2437 - acc: 0.5354\n",
      "Epoch 40/100\n",
      "8101/8101 [==============================] - 0s 27us/step - loss: 1.2433 - acc: 0.5354\n",
      "Epoch 41/100\n",
      "8101/8101 [==============================] - 0s 27us/step - loss: 1.2386 - acc: 0.5360\n",
      "Epoch 42/100\n",
      "8101/8101 [==============================] - 0s 28us/step - loss: 1.2362 - acc: 0.5343\n",
      "Epoch 43/100\n",
      "8101/8101 [==============================] - 0s 28us/step - loss: 1.2349 - acc: 0.5366\n",
      "Epoch 44/100\n",
      "8101/8101 [==============================] - 0s 27us/step - loss: 1.2293 - acc: 0.5370\n",
      "Epoch 45/100\n",
      "8101/8101 [==============================] - 0s 28us/step - loss: 1.2262 - acc: 0.5376\n",
      "Epoch 46/100\n",
      "8101/8101 [==============================] - 0s 28us/step - loss: 1.2233 - acc: 0.5373\n",
      "Epoch 47/100\n",
      "8101/8101 [==============================] - 0s 28us/step - loss: 1.2187 - acc: 0.5377\n",
      "Epoch 48/100\n",
      "8101/8101 [==============================] - 0s 30us/step - loss: 1.2162 - acc: 0.5393\n",
      "Epoch 49/100\n",
      "8101/8101 [==============================] - 0s 30us/step - loss: 1.2127 - acc: 0.5402\n",
      "Epoch 50/100\n",
      "8101/8101 [==============================] - 0s 29us/step - loss: 1.2094 - acc: 0.5420\n",
      "Epoch 51/100\n",
      "8101/8101 [==============================] - 0s 27us/step - loss: 1.2045 - acc: 0.5435\n",
      "Epoch 52/100\n",
      "8101/8101 [==============================] - 0s 28us/step - loss: 1.2016 - acc: 0.5426\n",
      "Epoch 53/100\n",
      "8101/8101 [==============================] - 0s 27us/step - loss: 1.1974 - acc: 0.5454\n",
      "Epoch 54/100\n",
      "8101/8101 [==============================] - 0s 28us/step - loss: 1.1957 - acc: 0.5456\n",
      "Epoch 55/100\n",
      "8101/8101 [==============================] - 0s 28us/step - loss: 1.1908 - acc: 0.5471\n",
      "Epoch 56/100\n",
      "8101/8101 [==============================] - 0s 28us/step - loss: 1.1875 - acc: 0.5482\n",
      "Epoch 57/100\n",
      "8101/8101 [==============================] - 0s 27us/step - loss: 1.1848 - acc: 0.5480\n",
      "Epoch 58/100\n",
      "8101/8101 [==============================] - 0s 27us/step - loss: 1.1820 - acc: 0.5503\n",
      "Epoch 59/100\n",
      "8101/8101 [==============================] - 0s 28us/step - loss: 1.1790 - acc: 0.5501\n",
      "Epoch 60/100\n",
      "8101/8101 [==============================] - 0s 27us/step - loss: 1.1764 - acc: 0.5533\n",
      "Epoch 61/100\n",
      "8101/8101 [==============================] - 0s 28us/step - loss: 1.1742 - acc: 0.5524\n",
      "Epoch 62/100\n",
      "8101/8101 [==============================] - 0s 27us/step - loss: 1.1718 - acc: 0.5536\n",
      "Epoch 63/100\n",
      "8101/8101 [==============================] - 0s 27us/step - loss: 1.1707 - acc: 0.5572\n",
      "Epoch 64/100\n",
      "8101/8101 [==============================] - 0s 28us/step - loss: 1.1672 - acc: 0.5545\n",
      "Epoch 65/100\n",
      "8101/8101 [==============================] - 0s 28us/step - loss: 1.1649 - acc: 0.5581\n",
      "Epoch 66/100\n",
      "8101/8101 [==============================] - 0s 28us/step - loss: 1.1643 - acc: 0.5582\n",
      "Epoch 67/100\n",
      "8101/8101 [==============================] - 0s 28us/step - loss: 1.1623 - acc: 0.5570\n",
      "Epoch 68/100\n",
      "8101/8101 [==============================] - 0s 27us/step - loss: 1.1587 - acc: 0.5587\n",
      "Epoch 69/100\n",
      "8101/8101 [==============================] - 0s 27us/step - loss: 1.1588 - acc: 0.5582\n",
      "Epoch 70/100\n",
      "8101/8101 [==============================] - 0s 27us/step - loss: 1.1584 - acc: 0.5589\n",
      "Epoch 71/100\n",
      "8101/8101 [==============================] - 0s 27us/step - loss: 1.1564 - acc: 0.5607\n",
      "Epoch 72/100\n",
      "8101/8101 [==============================] - 0s 27us/step - loss: 1.1567 - acc: 0.5591\n",
      "Epoch 73/100\n",
      "8101/8101 [==============================] - 0s 27us/step - loss: 1.1548 - acc: 0.5577\n",
      "Epoch 74/100\n",
      "8101/8101 [==============================] - 0s 28us/step - loss: 1.1517 - acc: 0.5601\n",
      "Epoch 75/100\n",
      "8101/8101 [==============================] - 0s 28us/step - loss: 1.1523 - acc: 0.5604\n",
      "Epoch 76/100\n",
      "8101/8101 [==============================] - 0s 28us/step - loss: 1.1497 - acc: 0.5633\n",
      "Epoch 77/100\n",
      "8101/8101 [==============================] - 0s 28us/step - loss: 1.1485 - acc: 0.5614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/100\n",
      "8101/8101 [==============================] - 0s 27us/step - loss: 1.1468 - acc: 0.5624\n",
      "Epoch 79/100\n",
      "8101/8101 [==============================] - 0s 28us/step - loss: 1.1467 - acc: 0.5624\n",
      "Epoch 80/100\n",
      "8101/8101 [==============================] - 0s 27us/step - loss: 1.1464 - acc: 0.5640\n",
      "Epoch 81/100\n",
      "8101/8101 [==============================] - 0s 27us/step - loss: 1.1453 - acc: 0.5630\n",
      "Epoch 82/100\n",
      "8101/8101 [==============================] - 0s 28us/step - loss: 1.1434 - acc: 0.5638: 0s - loss: 1.1612 - acc: 0.5\n",
      "Epoch 83/100\n",
      "8101/8101 [==============================] - 0s 28us/step - loss: 1.1420 - acc: 0.5610\n",
      "Epoch 84/100\n",
      "8101/8101 [==============================] - 0s 27us/step - loss: 1.1406 - acc: 0.5629\n",
      "Epoch 85/100\n",
      "8101/8101 [==============================] - 0s 27us/step - loss: 1.1400 - acc: 0.5654\n",
      "Epoch 86/100\n",
      "8101/8101 [==============================] - 0s 27us/step - loss: 1.1400 - acc: 0.5635\n",
      "Epoch 87/100\n",
      "8101/8101 [==============================] - 0s 27us/step - loss: 1.1387 - acc: 0.5660\n",
      "Epoch 88/100\n",
      "8101/8101 [==============================] - 0s 28us/step - loss: 1.1383 - acc: 0.5646\n",
      "Epoch 89/100\n",
      "8101/8101 [==============================] - 0s 28us/step - loss: 1.1360 - acc: 0.5666\n",
      "Epoch 90/100\n",
      "8101/8101 [==============================] - 0s 27us/step - loss: 1.1347 - acc: 0.5659\n",
      "Epoch 91/100\n",
      "8101/8101 [==============================] - 0s 27us/step - loss: 1.1341 - acc: 0.5660\n",
      "Epoch 92/100\n",
      "8101/8101 [==============================] - 0s 27us/step - loss: 1.1349 - acc: 0.5663\n",
      "Epoch 93/100\n",
      "8101/8101 [==============================] - 0s 27us/step - loss: 1.1358 - acc: 0.5666\n",
      "Epoch 94/100\n",
      "8101/8101 [==============================] - 0s 27us/step - loss: 1.1335 - acc: 0.5676\n",
      "Epoch 95/100\n",
      "8101/8101 [==============================] - 0s 27us/step - loss: 1.1326 - acc: 0.5696\n",
      "Epoch 96/100\n",
      "8101/8101 [==============================] - 0s 27us/step - loss: 1.1330 - acc: 0.5675\n",
      "Epoch 97/100\n",
      "8101/8101 [==============================] - 0s 27us/step - loss: 1.1315 - acc: 0.5670\n",
      "Epoch 98/100\n",
      "8101/8101 [==============================] - 0s 28us/step - loss: 1.1308 - acc: 0.5688\n",
      "Epoch 99/100\n",
      "8101/8101 [==============================] - 0s 27us/step - loss: 1.1310 - acc: 0.5676\n",
      "Epoch 100/100\n",
      "8101/8101 [==============================] - 0s 27us/step - loss: 1.1297 - acc: 0.5698\n",
      "[1.9972652e-02 8.8745192e-02 6.1883014e-01 1.9042654e-09 2.1150680e-01\n",
      " 6.0945250e-02] [0. 0. 1. 0. 0. 0.]\n",
      "[1.5102871e-01 3.9345458e-01 3.7165615e-01 2.5398099e-09 6.3361868e-02\n",
      " 2.0498767e-02] [0. 0. 0. 0. 0. 1.]\n",
      "[2.8030446e-01 3.3208326e-01 3.3101001e-01 2.6873788e-09 3.8922634e-02\n",
      " 1.7679671e-02] [0. 0. 1. 0. 0. 0.]\n",
      "[1.5776409e-02 9.3991198e-02 5.7238370e-01 8.4391155e-10 2.4882199e-01\n",
      " 6.9026671e-02] [0. 0. 0. 0. 1. 0.]\n",
      "[2.4370968e-02 1.3768849e-01 5.4646838e-01 1.3196186e-09 2.2534569e-01\n",
      " 6.6126473e-02] [0. 0. 1. 0. 0. 0.]\n",
      "[8.0333106e-02 1.8549804e-01 5.8684319e-01 1.3219988e-09 1.0687331e-01\n",
      " 4.0452316e-02] [0. 0. 1. 0. 0. 0.]\n",
      "[1.6002160e-02 8.5619256e-02 6.0313690e-01 1.4481688e-09 2.4867207e-01\n",
      " 4.6569660e-02] [0. 0. 0. 0. 1. 0.]\n",
      "[2.3674699e-02 1.3821948e-01 5.9865528e-01 2.8688798e-09 1.8918866e-01\n",
      " 5.0261825e-02] [1. 0. 0. 0. 0. 0.]\n",
      "[7.5555337e-03 6.6679507e-02 6.3655382e-01 9.9305980e-11 2.4876666e-01\n",
      " 4.0444497e-02] [0. 0. 1. 0. 0. 0.]\n",
      "[1.4701861e-01 2.8095677e-01 4.6962801e-01 5.4608429e-10 9.3282327e-02\n",
      " 9.1143390e-03] [0. 0. 1. 0. 0. 0.]\n",
      "[2.5463453e-02 1.1516990e-01 6.5262854e-01 1.8161661e-09 1.6745512e-01\n",
      " 3.9282899e-02] [0. 0. 0. 0. 0. 1.]\n",
      "[2.4709649e-02 1.3475460e-01 6.0918397e-01 2.0439537e-09 1.7541781e-01\n",
      " 5.5934049e-02] [0. 1. 0. 0. 0. 0.]\n",
      "[6.2631126e-03 5.1067922e-02 6.0935134e-01 1.8989126e-09 2.4591936e-01\n",
      " 8.7398350e-02] [0. 1. 0. 0. 0. 0.]\n",
      "[1.6959166e-02 9.9686533e-02 5.9849679e-01 2.3290008e-09 2.2217044e-01\n",
      " 6.2687062e-02] [0. 0. 1. 0. 0. 0.]\n",
      "[2.0099452e-02 1.2853728e-01 6.0010147e-01 7.3503054e-10 2.2045466e-01\n",
      " 3.0807080e-02] [0. 0. 1. 0. 0. 0.]\n",
      "[3.2157641e-02 1.9007605e-01 5.5405593e-01 4.6828417e-09 1.7597997e-01\n",
      " 4.7730330e-02] [0. 0. 0. 0. 1. 0.]\n",
      "[1.2059336e-02 1.0378699e-01 5.8278167e-01 2.6686364e-09 2.4767384e-01\n",
      " 5.3698141e-02] [0. 0. 0. 0. 1. 0.]\n",
      "[1.4209291e-02 1.0619719e-01 6.0421675e-01 1.4151883e-09 2.3790090e-01\n",
      " 3.7475888e-02] [0. 0. 1. 0. 0. 0.]\n",
      "[2.9925836e-02 1.3521184e-01 5.9582615e-01 3.0249638e-09 1.7951562e-01\n",
      " 5.9520453e-02] [0. 1. 0. 0. 0. 0.]\n",
      "[2.2120394e-02 1.1248139e-01 6.2627023e-01 2.4279884e-10 2.0911501e-01\n",
      " 3.0012915e-02] [0. 0. 1. 0. 0. 0.]\n",
      "---------------\n",
      "0.49259624876604147   [ 0.16666666666666666 ]\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from numpy import hstack\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy.random import rand\n",
    "from numpy.random import randn\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from matplotlib import pyplot\n",
    "\n",
    "\n",
    "def define_NN(input_dim, n_outputs):\n",
    "    \n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(6*(input_dim+n_outputs), activation='sigmoid', kernel_initializer='he_uniform', input_dim=input_dim))\n",
    "\t#model.add(Dense(10*n_outputs, activation='sigmoid', kernel_initializer='he_uniform', input_dim=input_dim))\n",
    "\tmodel.add(Dense(n_outputs, activation='softmax'))\n",
    "\treturn model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def prepare_data():\n",
    "\n",
    "    rdata0 = pd.read_csv(\"BankChurners.csv\")\n",
    "    rdata = rdata0.drop([rdata0.columns[0],rdata0.columns[-1],rdata0.columns[-2]],axis=1)\n",
    "    rdata.head()\n",
    "\n",
    "    Dicts = []\n",
    "    for c in range(len(rdata.columns)):\n",
    "        l1 = rdata[rdata.columns[c]]\n",
    "        l2 = list(dict.fromkeys(l1))\n",
    "        l2.sort()\n",
    "        N = len(l2)-1\n",
    "        Dicts.append(dict())\n",
    "        for i in range(len(l2)):\n",
    "            Dicts[c] [l2[i]] = i / N \n",
    "\n",
    "\n",
    "    for c in range(len(rdata.columns)):\n",
    "        print (rdata.columns[c],':', len(Dicts[c]))\n",
    "    #    print (\" \")\n",
    "\n",
    "\n",
    "    print (rdata.shape)\n",
    "\n",
    "    DArray = np.zeros(  (rdata.shape[0], rdata.shape[1]),dtype = \"float\")\n",
    "    print (DArray.shape)\n",
    "\n",
    "    for n in range(rdata.shape[0]):\n",
    "        for c in range(rdata.shape[1]):\n",
    "            #print (rdata [n,c])\n",
    "            DArray[n,c] =  Dicts[c] [ rdata.iat [n,c]]\n",
    "\n",
    "    #print (DArray[1:20,3:6])\n",
    "    CCnt = []\n",
    "    for c in range(len(rdata.columns)):\n",
    "        CCnt.append(len(Dicts[c]))\n",
    "    return (DArray,rdata.columns,CCnt)\n",
    "\n",
    "\n",
    "(AData, cnames, CCnt) = prepare_data()\n",
    "\n",
    "Qdim = 3\n",
    "Qlim = 0\n",
    "dc = 1. / (CCnt[Qdim]-1)\n",
    "print (\"Attribute \", cnames[Qdim],\"dc=\",dc)\n",
    "\n",
    "latent_dim = AData.shape[1]-1\n",
    "target_dim = CCnt[Qdim]\n",
    "\n",
    "\n",
    "N = int(AData.shape[0]*.8)\n",
    "train_data_X = np.zeros((N, latent_dim))\n",
    "train_data_Y = np.zeros((N, target_dim))\n",
    "for i in range(N):\n",
    "    c = 0\n",
    "    for j in range(latent_dim+1):\n",
    "        if j != Qdim:\n",
    "            train_data_X[i,c] = AData[i,j]\n",
    "            c = c + 1\n",
    "        else:\n",
    "            train_data_Y[i,int(AData[i,j]/dc)] = 1\n",
    "\n",
    "    \n",
    "NN = define_NN(latent_dim, target_dim)\n",
    "NN.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "NN.fit(train_data_X,train_data_Y,verbose=1,epochs=100)\n",
    "\n",
    "\n",
    "N2 = AData.shape[0] - N\n",
    "test_data_X = np.zeros((N2, latent_dim))\n",
    "test_data_Y = np.zeros((N2, target_dim))\n",
    "for i in range(N2):\n",
    "    c = 0\n",
    "    for j in range(latent_dim+1):\n",
    "        if j != Qdim:\n",
    "            test_data_X[i,c] = AData[N  + i,j]\n",
    "            c = c + 1\n",
    "        else:\n",
    "            test_data_Y[i,int(AData[i,j]/dc)] = 1\n",
    "\n",
    "        \n",
    "\n",
    "pred_data_Y = NN.predict(test_data_X)\n",
    "\n",
    "Diff = []\n",
    "for i in range(N2):\n",
    "    tl = list(test_data_Y[i])\n",
    "    maxi = tl.index(max(tl))\n",
    "    mval = pred_data_Y[i][maxi]\n",
    "    db = 0\n",
    "    for j in range(target_dim):\n",
    "        if pred_data_Y[i][j] > mval:\n",
    "            db = db + 1\n",
    "    if db <= Qlim:\n",
    "        Diff.append(1)\n",
    "    else:\n",
    "        Diff.append(0)\n",
    "    \n",
    "\n",
    "for i in range(20):    \n",
    "    print (pred_data_Y[i],  test_data_Y[i])\n",
    "print (\"---------------\")\n",
    "print (sum(Diff) / len(Diff), \"  [\", 1/CCnt[Qdim],\"]\")\n",
    "print (\"---------------\")\n",
    "#print (Diff)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attrition_Flag : 2\n",
      "Customer_Age : 45\n",
      "Gender : 2\n",
      "Dependent_count : 6\n",
      "Education_Level : 7\n",
      "Marital_Status : 4\n",
      "Income_Category : 6\n",
      "Card_Category : 4\n",
      "Months_on_book : 44\n",
      "Total_Relationship_Count : 6\n",
      "Months_Inactive_12_mon : 7\n",
      "Contacts_Count_12_mon : 7\n",
      "Credit_Limit : 6205\n",
      "Total_Revolving_Bal : 1974\n",
      "Avg_Open_To_Buy : 6813\n",
      "Total_Amt_Chng_Q4_Q1 : 1158\n",
      "Total_Trans_Amt : 5033\n",
      "Total_Trans_Ct : 126\n",
      "Total_Ct_Chng_Q4_Q1 : 830\n",
      "Avg_Utilization_Ratio : 964\n",
      "(10127, 20)\n",
      "(10127, 20)\n",
      "Attribute  Dependent_count\n",
      "Epoch 1/30\n",
      "8101/8101 [==============================] - 1s 161us/step - loss: 0.0856 - mean_squared_error: 0.0856\n",
      "Epoch 2/30\n",
      "8101/8101 [==============================] - 0s 25us/step - loss: 0.0680 - mean_squared_error: 0.0680\n",
      "Epoch 3/30\n",
      "8101/8101 [==============================] - 0s 24us/step - loss: 0.0635 - mean_squared_error: 0.0635\n",
      "Epoch 4/30\n",
      "8101/8101 [==============================] - 0s 25us/step - loss: 0.0593 - mean_squared_error: 0.0593\n",
      "Epoch 5/30\n",
      "8101/8101 [==============================] - 0s 24us/step - loss: 0.0564 - mean_squared_error: 0.0564\n",
      "Epoch 6/30\n",
      "8101/8101 [==============================] - 0s 25us/step - loss: 0.0546 - mean_squared_error: 0.0546\n",
      "Epoch 7/30\n",
      "8101/8101 [==============================] - 0s 25us/step - loss: 0.0526 - mean_squared_error: 0.0526\n",
      "Epoch 8/30\n",
      "8101/8101 [==============================] - 0s 25us/step - loss: 0.0518 - mean_squared_error: 0.0518\n",
      "Epoch 9/30\n",
      "8101/8101 [==============================] - 0s 24us/step - loss: 0.0507 - mean_squared_error: 0.0507\n",
      "Epoch 10/30\n",
      "8101/8101 [==============================] - 0s 24us/step - loss: 0.0502 - mean_squared_error: 0.0502\n",
      "Epoch 11/30\n",
      "8101/8101 [==============================] - 0s 25us/step - loss: 0.0497 - mean_squared_error: 0.0497\n",
      "Epoch 12/30\n",
      "8101/8101 [==============================] - 0s 25us/step - loss: 0.0496 - mean_squared_error: 0.0496\n",
      "Epoch 13/30\n",
      "8101/8101 [==============================] - 0s 24us/step - loss: 0.0491 - mean_squared_error: 0.0491\n",
      "Epoch 14/30\n",
      "8101/8101 [==============================] - 0s 24us/step - loss: 0.0489 - mean_squared_error: 0.0489\n",
      "Epoch 15/30\n",
      "8101/8101 [==============================] - 0s 24us/step - loss: 0.0490 - mean_squared_error: 0.0490\n",
      "Epoch 16/30\n",
      "8101/8101 [==============================] - 0s 24us/step - loss: 0.0485 - mean_squared_error: 0.0485\n",
      "Epoch 17/30\n",
      "8101/8101 [==============================] - 0s 25us/step - loss: 0.0481 - mean_squared_error: 0.0481\n",
      "Epoch 18/30\n",
      "8101/8101 [==============================] - 0s 25us/step - loss: 0.0483 - mean_squared_error: 0.0483\n",
      "Epoch 19/30\n",
      "8101/8101 [==============================] - 0s 25us/step - loss: 0.0478 - mean_squared_error: 0.0478\n",
      "Epoch 20/30\n",
      "8101/8101 [==============================] - 0s 25us/step - loss: 0.0480 - mean_squared_error: 0.0480\n",
      "Epoch 21/30\n",
      "8101/8101 [==============================] - 0s 25us/step - loss: 0.0478 - mean_squared_error: 0.0478\n",
      "Epoch 22/30\n",
      "8101/8101 [==============================] - 0s 25us/step - loss: 0.0474 - mean_squared_error: 0.0474\n",
      "Epoch 23/30\n",
      "8101/8101 [==============================] - 0s 25us/step - loss: 0.0478 - mean_squared_error: 0.0478\n",
      "Epoch 24/30\n",
      "8101/8101 [==============================] - 0s 25us/step - loss: 0.0474 - mean_squared_error: 0.0474\n",
      "Epoch 25/30\n",
      "8101/8101 [==============================] - 0s 24us/step - loss: 0.0472 - mean_squared_error: 0.0472\n",
      "Epoch 26/30\n",
      "8101/8101 [==============================] - 0s 25us/step - loss: 0.0471 - mean_squared_error: 0.0471\n",
      "Epoch 27/30\n",
      "8101/8101 [==============================] - 0s 25us/step - loss: 0.0474 - mean_squared_error: 0.0474\n",
      "Epoch 28/30\n",
      "8101/8101 [==============================] - 0s 25us/step - loss: 0.0472 - mean_squared_error: 0.0472\n",
      "Epoch 29/30\n",
      "8101/8101 [==============================] - 0s 24us/step - loss: 0.0471 - mean_squared_error: 0.0471\n",
      "Epoch 30/30\n",
      "8101/8101 [==============================] - 0s 25us/step - loss: 0.0471 - mean_squared_error: 0.0471\n",
      "dc= 0.1\n",
      "[0.59149677] [0.8] 0\n",
      "[0.29762292] [0.6] 0\n",
      "[0.2388976] [0.] 0\n",
      "[0.5798004] [0.6] 1\n",
      "[0.5327565] [0.6] 1\n",
      "[0.42412317] [0.8] 0\n",
      "[0.54388493] [0.4] 0\n",
      "[0.5369651] [0.6] 1\n",
      "[0.6019744] [0.4] 0\n",
      "[0.36978] [0.2] 0\n",
      "[0.54290456] [1.] 0\n",
      "[0.4039412] [0.6] 0\n",
      "[0.64730996] [0.8] 0\n",
      "[0.607737] [1.] 0\n",
      "[0.5153346] [0.8] 0\n",
      "[0.5097461] [0.6] 1\n",
      "[0.56725067] [0.4] 0\n",
      "[0.45728326] [0.4] 1\n",
      "[0.50014675] [0.2] 0\n",
      "[0.5195435] [0.8] 0\n",
      "---------------\n",
      "0.34106614017769005   [ 0.16666666666666666 ]\n",
      "---------------\n",
      "[0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from numpy import hstack\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy.random import rand\n",
    "from numpy.random import randn\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from matplotlib import pyplot\n",
    "\n",
    "\n",
    "def define_NN(input_dim, n_outputs):\n",
    "    \n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(3*(input_dim+n_outputs), activation='relu', kernel_initializer='he_uniform', input_dim=input_dim))\n",
    "\tmodel.add(Dense(10*n_outputs, activation='relu', kernel_initializer='he_uniform', input_dim=input_dim))\n",
    "\tmodel.add(Dense(n_outputs, activation='linear'))\n",
    "\treturn model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def prepare_data():\n",
    "\n",
    "    rdata0 = pd.read_csv(\"BankChurners.csv\")\n",
    "    rdata = rdata0.drop([rdata0.columns[0],rdata0.columns[-1],rdata0.columns[-2]],axis=1)\n",
    "    rdata.head()\n",
    "\n",
    "    Dicts = []\n",
    "    for c in range(len(rdata.columns)):\n",
    "        l1 = rdata[rdata.columns[c]]\n",
    "        l2 = list(dict.fromkeys(l1))\n",
    "        l2.sort()\n",
    "        N = len(l2)-1\n",
    "        Dicts.append(dict())\n",
    "        for i in range(len(l2)):\n",
    "            Dicts[c] [l2[i]] = i / N \n",
    "\n",
    "\n",
    "    for c in range(len(rdata.columns)):\n",
    "        print (rdata.columns[c],':', len(Dicts[c]))\n",
    "    #    print (\" \")\n",
    "\n",
    "\n",
    "    print (rdata.shape)\n",
    "\n",
    "    DArray = np.zeros(  (rdata.shape[0], rdata.shape[1]),dtype = \"float\")\n",
    "    print (DArray.shape)\n",
    "\n",
    "    for n in range(rdata.shape[0]):\n",
    "        for c in range(rdata.shape[1]):\n",
    "            #print (rdata [n,c])\n",
    "            DArray[n,c] =  Dicts[c] [ rdata.iat [n,c]]\n",
    "\n",
    "    #print (DArray[1:20,3:6])\n",
    "    CCnt = []\n",
    "    for c in range(len(rdata.columns)):\n",
    "        CCnt.append(len(Dicts[c]))\n",
    "    return (DArray,rdata.columns,CCnt)\n",
    "\n",
    "\n",
    "(AData, cnames, CCnt) = prepare_data()\n",
    "\n",
    "Qdim = 3\n",
    "Qlim = 0\n",
    "print (\"Attribute \", cnames[Qdim])\n",
    "\n",
    "latent_dim = AData.shape[1]-1\n",
    "target_dim = 1\n",
    "\n",
    "\n",
    "N = int(AData.shape[0]*.8)\n",
    "train_data_X = np.zeros((N, latent_dim))\n",
    "train_data_Y = np.zeros((N, target_dim))\n",
    "for i in range(N):\n",
    "    c = 0\n",
    "    for j in range(latent_dim+1):\n",
    "        if j != Qdim:\n",
    "            train_data_X[i,c] = AData[i,j]\n",
    "            c = c + 1\n",
    "        else:\n",
    "            train_data_Y[i,0] = AData[i,j]\n",
    "\n",
    "    \n",
    "NN = define_NN(latent_dim, target_dim)\n",
    "NN.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse'])\n",
    "NN.fit(train_data_X,train_data_Y,verbose=1,epochs=30)\n",
    "\n",
    "\n",
    "N2 = AData.shape[0] - N\n",
    "test_data_X = np.zeros((N2, latent_dim))\n",
    "test_data_Y = np.zeros((N2, target_dim))\n",
    "for i in range(N2):\n",
    "    c = 0\n",
    "    for j in range(latent_dim+1):\n",
    "        if j != Qdim:\n",
    "            test_data_X[i,c] = AData[N  + i,j]\n",
    "            c = c + 1\n",
    "        else:\n",
    "            test_data_Y[i,0] = AData[N  + i,j]\n",
    "\n",
    "\n",
    "pred_data_Y = NN.predict(test_data_X)\n",
    "\n",
    "Diff = []\n",
    "dc = (1. / (CCnt[Qdim]-1))*.5\n",
    "print (\"dc=\",dc)\n",
    "for i in range(N2):\n",
    "    d = abs(pred_data_Y[i] - test_data_Y[i])\n",
    "    db = 0\n",
    "    while d > dc*(1 + 2*db):\n",
    "        db = db + 1\n",
    "    if db <= Qlim:\n",
    "        Diff.append(1)\n",
    "    else:\n",
    "        Diff.append(0)\n",
    "\n",
    "\n",
    "for i in range(20):    \n",
    "    print (pred_data_Y[i],  test_data_Y[i], Diff[i])\n",
    "print (\"---------------\")\n",
    "print (sum(Diff) / len(Diff), \"  [\", 1/CCnt[Qdim],\"]\")\n",
    "print (\"---------------\")\n",
    "#print (Diff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 33, 3: 76, 6: 17}\n",
      "[(1, 33), (3, 76), (6, 17)]\n",
      "[(6, 17), (1, 33), (3, 76)]\n"
     ]
    }
   ],
   "source": [
    "x = dict()\n",
    "x[1] = 33\n",
    "x[3] = 76\n",
    "x[6] = 17\n",
    "print (x)\n",
    "y = [(k,x[k]) for k in x.keys()]\n",
    "print (y)\n",
    "y.sort(key=lambda z : z[1])\n",
    "print (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9480385759645931\n"
     ]
    }
   ],
   "source": [
    "print (random.random())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
